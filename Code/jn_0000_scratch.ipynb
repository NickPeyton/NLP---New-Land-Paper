{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679bc125",
   "metadata": {},
   "source": [
    "# Testbed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819aa54d",
   "metadata": {},
   "source": [
    "Ones that work:\n",
    "- Qwen\n",
    "- Ovis\n",
    "- EasyOCR\n",
    "- TrOCR\n",
    "- PyTesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721a3cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen3VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor, AutoModel, AutoImageProcessor, AutoModelForCausalLM, TrOCRProcessor, VisionEncoderDecoderModel, default_data_collator, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pytesseract as pt\n",
    "import easyocr\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0420312a",
   "metadata": {},
   "source": [
    "### Ovis 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f308ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      4\u001b[39m max_new_tokens = \u001b[32m64\u001b[39m\n\u001b[32m      5\u001b[39m ovis_quantization_config = BitsAndBytesConfig(\n\u001b[32m      6\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m     bnb_4bit_compute_dtype=torch.bfloat16,\n\u001b[32m      8\u001b[39m     bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      9\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43movis_quantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m.eval().cuda()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\PhD\\DissolutionProgramming\\NLP---New-Land-Paper\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:597\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    595\u001b[39m         model_class.register_for_auto_class(auto_class=\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    596\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping:\n\u001b[32m    601\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\PhD\\DissolutionProgramming\\NLP---New-Land-Paper\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\PhD\\DissolutionProgramming\\NLP---New-Land-Paper\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:5029\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5027\u001b[39m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[32m   5028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5029\u001b[39m     device_map = \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5031\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   5032\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\PhD\\DissolutionProgramming\\NLP---New-Land-Paper\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:1365\u001b[39m, in \u001b[36m_get_device_map\u001b[39m\u001b[34m(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\u001b[39m\n\u001b[32m   1362\u001b[39m     device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m         \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1368\u001b[39m     tied_params = find_tied_parameters(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\PhD\\DissolutionProgramming\\NLP---New-Land-Paper\\.venv\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:127\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    128\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`from_pretrained`. Check \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfor more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"AIDC-AI/Ovis2.5-9B\"\n",
    "\n",
    "# Total tokens for thinking + answer. Ensure: max_new_tokens > thinking_budget + 25\n",
    "max_new_tokens = 64\n",
    "ovis_quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=ovis_quantization_config,\n",
    "    device_map=\"auto\"\n",
    "    \n",
    ").eval().cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f97cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Richard Langworthie G 3 1/3\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(\"C:\\\\PhD\\\\DissolutionProgramming\\\\NLP---New-Land-Paper\\\\Data\\\\Processed\\\\subsidy1524\\\\value_lines\\\\value_12.png\")\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\", \"image\": img},\n",
    "        {\"type\": \"text\", \"text\": \"OCR this, outputting ONLY the text. Be very careful not to accidentally misidentify a fraction as a numeral. Fractions will be one small number on top of another with no line.\"},\n",
    "    ],\n",
    "}]\n",
    "\n",
    "input_ids, pixel_values, grid_thws = model.preprocess_inputs(\n",
    "    messages=messages,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False\n",
    ")\n",
    "input_ids = input_ids.cuda()\n",
    "pixel_values = pixel_values.cuda() if pixel_values is not None else None\n",
    "grid_thws = grid_thws.cuda() if grid_thws is not None else None\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs=input_ids,\n",
    "    pixel_values=pixel_values,\n",
    "    grid_thws=grid_thws,\n",
    "    enable_thinking=False,\n",
    "    max_new_tokens=max_new_tokens,\n",
    ")\n",
    "\n",
    "response = model.text_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f10b70",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a148d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae181e1642a47e290dc211d92d1c055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Qwen\n",
    "qwen_quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", # Use the standard model, BNB quantizes it on the fly\n",
    "    quantization_config=qwen_quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\"\n",
    ").eval().cuda()\n",
    "\n",
    "qwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", use_fast=True)\n",
    "print(\"Qwen model loaded.\")\n",
    "\n",
    "# Pytesseract\n",
    "pt.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "pt_config = r'--oem 3 --psm 7 -c tessedit_char_whitelist= 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ/ '\n",
    "\n",
    "# EasyOCR\n",
    "eo_model = easyocr.Reader(['en'])\n",
    "\n",
    "# TrOCR\n",
    "trocr_model_id = \"microsoft/trocr-large-printed\"\n",
    "\n",
    "trocr_quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    llm_int8_skip_modules=[\"pooler\"] \n",
    ")\n",
    "\n",
    "trocr_model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    trocr_model_id,\n",
    "    quantization_config=trocr_quant_config,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "trocr_processor = TrOCRProcessor.from_pretrained(trocr_model_id)\n",
    "trocr_tokenizer = AutoTokenizer.from_pretrained(trocr_model_id)\n",
    "trocr_model.to(\"cuda\")\n",
    "print('All models loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4672a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_config = r'--oem 3 --psm 7 -c tessedit_char_whitelist= 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ/ '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40cc39d",
   "metadata": {},
   "source": [
    "### Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a10beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen VL OCR Text: Amy at Ven wid G 2\n",
      "EasyOCR OCR Text: at Ven wid G   2 Amy\n",
      "TrOCR OCR Text: AMY AT VEN WID G 2\n",
      "Pytesseract OCR Text: Amy at Ven wid G 2\n",
      "Total Time Taken: 4.59190034866333 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: John Couse G 4\n",
      "EasyOCR OCR Text: John Couse 6\n",
      "TrOCR OCR Text: JOHH COUSE G 4\n",
      "Pytesseract OCR Text: John Couse G 4\n",
      "Total Time Taken: 2.602468490600586 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: Alice Full G.\n",
      "EasyOCR OCR Text: Alice Full G\n",
      "TrOCR OCR Text: ALICE FULL ORIG.\n",
      "Pytesseract OCR Text: Alice Full G .\n",
      "Total Time Taken: 1.9561901092529297 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: Wm Man of Boldburgh G 83\n",
      "EasyOCR OCR Text: Wm Man of Boldburgh G 8 3\n",
      "TrOCR OCR Text: WM MAN OF BOLDBURGH G 85\n",
      "Pytesseract OCR Text: Wm Man of Boldburgh G 83\n",
      "Total Time Taken: 2.89148211479187 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: Richard Langworthie G 3½\n",
      "EasyOCR OCR Text: 1 Richard Langworthie G 33\n",
      "TrOCR OCR Text: RICHARD LANGWORTHIE G 3%\n",
      "Pytesseract OCR Text: Richard Langworthie G 3s\n",
      "Total Time Taken: 2.8877251148223877 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: Roger Mylward sen G 35\n",
      "EasyOCR OCR Text: Roger Mylward sen G 35\n",
      "TrOCR OCR Text: ROGER MYLWARD SEN G 35\n",
      "Pytesseract OCR Text: Roger Mylward sen G 35\n",
      "Total Time Taken: 2.8842852115631104 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: In Wyndyate of Grendin G11\n",
      "EasyOCR OCR Text: Jn Wyndyate of Grendin 611\n",
      "TrOCR OCR Text: JN WYNDYATE OF GRENDIN G11\n",
      "Pytesseract OCR Text: Jn Wyndyate of Grendin G11\n",
      "Total Time Taken: 3.6859829425811768 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: Robert Noseworthy G 3²/₃\n",
      "EasyOCR OCR Text: Robert Naseworthie 6 33\n",
      "TrOCR OCR Text: ROBERT NOSEWORTHIE G 3E\n",
      "Pytesseract OCR Text: Robert Naoseworthie G 32\n",
      "Total Time Taken: 3.112718343734741 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: Richard Man jun G 31/3\n",
      "EasyOCR OCR Text: Richard Man jun G 33\n",
      "TrOCR OCR Text: RICHARD MAN JUN G 3J\n",
      "Pytesseract OCR Text: Richard Man jun GC 33%\n",
      "Total Time Taken: 4.687366008758545 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: Roger Nycoll . G 12/3\n",
      "EasyOCR OCR Text: Roger Nycoll G 13\n",
      "TrOCR OCR Text: ROGER NYCOLL . G 13\n",
      "Pytesseract OCR Text: Roger Nycoll : G 12\n",
      "Total Time Taken: 5.1202874183654785 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: William Dey G 3\n",
      "EasyOCR OCR Text: William G   3 Dey\n",
      "TrOCR OCR Text: WILLIAM DEY G 3\n",
      "Pytesseract OCR Text: William Dey G 3\n",
      "Total Time Taken: 2.815765619277954 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: ratyd and payd to my lord Cardynall £12.10.0[L]00 -\n",
      "EasyOCR OCR Text: ratyd and payd to my lord Cardynall E12.10.0(LIOO\n",
      "TrOCR OCR Text: CATAN AND PAYT TO MY LOOD\n",
      "Pytesseract OCR Text: \n",
      "Total Time Taken: 8.839818239212036 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: Nicholas Payn G 3½\n",
      "EasyOCR OCR Text: Nicholas Payn G 33\n",
      "TrOCR OCR Text: NICHOLAS PAYM G 3%\n",
      "Pytesseract OCR Text: Nicholas Payn G 33\n",
      "Total Time Taken: 4.091825485229492 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: John Smeth G 2\n",
      "EasyOCR OCR Text: John Smeth G   2,\n",
      "TrOCR OCR Text: JOHN SMETH G 2\n",
      "Pytesseract OCR Text: John Smeth GC 2\n",
      "Total Time Taken: 3.3923473358154297 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: Haselett French wid(G8)G10\n",
      "EasyOCR OCR Text: Hase lett French wid(G8 JGlO\n",
      "TrOCR OCR Text: HASELETT FRENCH WID(G8)G10\n",
      "Pytesseract OCR Text: Haselett French wid (G8)G10\n",
      "Total Time Taken: 5.801522493362427 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: Roger Cole G 4\n",
      "EasyOCR OCR Text: Roger Cole G\n",
      "TrOCR OCR Text: ROGER COLE G 4\n",
      "Pytesseract OCR Text: Roger Cole GC 4\n",
      "Total Time Taken: 3.249305248260498 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: William Bartlett G 3½\n",
      "EasyOCR OCR Text: William Bartlett G 33\n",
      "TrOCR OCR Text: WILLIAM BARTEIT G 3J\n",
      "Pytesseract OCR Text: William Bartlett G 33\n",
      "Total Time Taken: 4.096088409423828 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: John Speke G 3⅓\n",
      "EasyOCR OCR Text: John Speke G 33\n",
      "TrOCR OCR Text: JOHN SPEKE G 3J\n",
      "Pytesseract OCR Text: John Speke G 33\n",
      "Total Time Taken: 3.790851593017578 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: William Vennyng G 11\n",
      "EasyOCR OCR Text: William Vennyng G l1\n",
      "TrOCR OCR Text: WILLIAM VENNYNG G 11\n",
      "Pytesseract OCR Text: William Vennyng G 11\n",
      "Total Time Taken: 3.8517508506774902 seconds\n",
      "------------------------------------------------\n",
      "\n",
      "Qwen VL OCR Text: Andrew Gelerd G 3²/₃\n",
      "EasyOCR OCR Text: Andrew Gelerd G 33\n",
      "TrOCR OCR Text: ANDREW GELEND E G 3E\n",
      "Pytesseract OCR Text: Andrew Gelerd G 32\n",
      "Total Time Taken: 4.30320143699646 seconds\n",
      "------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for img_path in os.listdir(\"C:\\\\PhD\\\\DissolutionProgramming\\\\NLP---New-Land-Paper\\\\Data\\\\Processed\\\\subsidy1524\\\\value_lines\"):\n",
    "    start_time = time.time()\n",
    "    img = Image.open(os.path.join(\"C:\\\\PhD\\\\DissolutionProgramming\\\\NLP---New-Land-Paper\\\\Data\\\\Processed\\\\subsidy1524\\\\value_lines\", img_path))\n",
    "    # Qwen\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": img,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": \"OCR this, outputting ONLY the text. Be very careful not to accidentally misidentify a fraction as a numeral. Fractions will be one small number on top of another with no line.\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = qwen_processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    generated_ids = qwen_model.generate(**inputs, max_new_tokens=64)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    qwen_text = qwen_processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "    # EasyOCR\n",
    "    img_array = np.array(img)\n",
    "    eo_text = eo_model.readtext(img_array)\n",
    "    eo_text = ' '.join([x[1] for x in eo_text]).strip()\n",
    "    #TrOCR\n",
    "    pixel_values = trocr_processor(img, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    generated_ids = trocr_model.generate(pixel_values)\n",
    "    generated_text = trocr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    tr_text = generated_text.strip()\n",
    "\n",
    "    # PyTesseract\n",
    "    pt_text = pt.image_to_string(img, config=pt_config).strip()\n",
    "    print(\"Qwen VL OCR Text:\"\n",
    "        , qwen_text)\n",
    "    print(\"EasyOCR OCR Text:\"\n",
    "        , eo_text)\n",
    "    print(\"TrOCR OCR Text:\"\n",
    "            , tr_text)\n",
    "    print(\"Pytesseract OCR Text:\"\n",
    "            , pt_text)\n",
    "    end_time = time.time()\n",
    "    print(f\"Total Time Taken: {end_time - start_time} seconds\")\n",
    "    print('------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fccddba",
   "metadata": {},
   "source": [
    "### EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cdf9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fce266b",
   "metadata": {},
   "source": [
    "### TrOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a898a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86600144",
   "metadata": {},
   "source": [
    "### PyTesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee1295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a977dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen VL OCR Text: ['Richard Langworthie G 3½']\n",
      "EasyOCR OCR Text: 1 Richard Langworthie G 33\n",
      "TrOCR OCR Text: RICHARD LANGWORTHIE G 3%\n",
      "Pytesseract OCR Text: Richard Langworthie G 3s\n",
      "Total Time Taken: 4.944117546081543 seconds\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
