{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a32d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import os\n",
    "import shutil\n",
    "import platform\n",
    "if platform.node() == 'Nick_Laptop':\n",
    "    drive = 'C'\n",
    "elif platform.node() == 'MSI':\n",
    "    drive = 'D'\n",
    "else:\n",
    "    drive = 'uhhhhhh'\n",
    "    print('Uhhhhhhhhhhhhh')\n",
    "os.chdir(f'{drive}:/PhD/DissolutionProgramming/LND---Land-Paper')\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import platform\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import pytesseract as pt\n",
    "import easyocr\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, TrOCRProcessor, VisionEncoderDecoderModel, default_data_collator, AutoTokenizer\n",
    "\n",
    "\n",
    "if platform.node() == 'Nick_Laptop':\n",
    "    drive = 'C'\n",
    "elif platform.node() == 'MSI':\n",
    "    drive = 'D'\n",
    "else:\n",
    "    print('oh shit oh fuck')\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "os.chdir(f'{drive}:/PhD/DissolutionProgramming/LND---Land-Paper')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "\n",
    "#%% Globals\n",
    "PROCESSED_DATA = 'Data/Processed'\n",
    "MODEL_FOLDER = f'Code/ML Models/trocr_model'\n",
    "#%% Load our models\n",
    "\n",
    "pt.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "eo_model = easyocr.Reader(['en'])\n",
    "model_size = 'small'\n",
    "#%% Getting Latest Model\n",
    "\n",
    "\n",
    "try:\n",
    "    checkpoint_dict = {}\n",
    "    if not os.path.isdir(MODEL_FOLDER):\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    for folder in os.listdir(MODEL_FOLDER):\n",
    "        modified_time = os.path.getmtime(f'{MODEL_FOLDER}/{folder}')\n",
    "        checkpoint_dict[modified_time] = folder\n",
    "    latest_checkpoint = checkpoint_dict[max(checkpoint_dict.keys())]\n",
    "    latest_model_location = os.path.join(MODEL_FOLDER, latest_checkpoint)\n",
    "    trocr_model = VisionEncoderDecoderModel.from_pretrained(latest_model_location,\n",
    "                                                      cache_dir=latest_model_location,\n",
    "                                                      local_files_only=True)\n",
    "    print('Latest Model Loaded!')\n",
    "    for folder in os.listdir(MODEL_FOLDER):\n",
    "        if folder != latest_checkpoint:\n",
    "            shutil.rmtree(f'{MODEL_FOLDER}/{folder}')\n",
    "    print('Older models vaporized!')\n",
    "except:\n",
    "\n",
    "    trocr_model = VisionEncoderDecoderModel.from_pretrained(f'microsoft/trocr-{model_size}-printed')\n",
    "    print('Model Downloaded')\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(f'microsoft/trocr-{model_size}-printed',\n",
    "                                           use_fast=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'microsoft/trocr-{model_size}-printed',\n",
    "                                          use_fast=False)\n",
    "print('Processor and tokenizer downloaded')\n",
    "\n",
    "trocr_model.to(device)\n",
    "\n",
    "#%% Creating the Two Lists\n",
    "\n",
    "page_pattern = re.compile(r'(\\d+,*)')\n",
    "see_also_pattern = re.compile(r' ?see ?also ?([A-Z][a-z]+)')\n",
    "see_pattern = re.compile(r' ?see ?([A-Z][a-z]+)')\n",
    "replacement = r', \\1'\n",
    "symbol_pattern = re.compile(r'[-_+=/\\\\<>?;:\\'\\\"!@#$%^&*|]')\n",
    "alt_spelling_pattern = re.compile(r'([A-Z][a-z]+)\\((\\w)\\)')\n",
    "replacement_two = r'\\1, \\1\\2'\n",
    "cont_pattern = re.compile(r'([A-Z][a-z]+ \\(cont\\.\\))')\n",
    "\n",
    "#%%\n",
    "for subsidy in [\n",
    "    1524,\n",
    "    1543,\n",
    "    1581,\n",
    "    # 1642,\n",
    "    # 1647,\n",
    "    # 1660,\n",
    "    # # 1661,\n",
    "    1674,\n",
    "     ]:\n",
    "    INDEX_FOLDER = f'{PROCESSED_DATA}/subsidy{subsidy}/processed_pages/surname_index'\n",
    "    surname_index_df = pd.DataFrame()\n",
    "    for image_file in tqdm(os.listdir(INDEX_FOLDER), total=len(os.listdir(INDEX_FOLDER))):\n",
    "        if not image_file.endswith('.png'):\n",
    "            continue\n",
    "        page = int(image_file[-7:-4])\n",
    "        start = time.time()\n",
    "        img = Image.open(f'{INDEX_FOLDER}/{image_file}')\n",
    "        img = img.crop((0, 400, img.size[0], img.size[1]))\n",
    "        img_array = np.array(img)\n",
    "        vert_proj = np.sum(img_array, axis=0)\n",
    "        projection_series = pd.Series(vert_proj)\n",
    "        projection_rolling = projection_series.rolling(window=100, center=True)\n",
    "        projection_smoothed = projection_rolling.mean()\n",
    "        projection_smoothed = projection_smoothed * -1 + 1_350_000\n",
    "        peaks, properties = signal.find_peaks(x=projection_smoothed,\n",
    "                                              height = 50_000,\n",
    "                                              distance=1000,\n",
    "                                              prominence = 10000)\n",
    "        peaks = [x - 100 for x in peaks]\n",
    "        peaks = peaks[1:]\n",
    "        # plot the peaks on the image\n",
    "        if page % 10 == 0:\n",
    "            plt.imshow(img_array, cmap='gray')\n",
    "            for peak in peaks:\n",
    "                plt.axvline(x=peak, color='r', linestyle='--')\n",
    "            plt.show()\n",
    "\n",
    "        cut_list = [0] + list(peaks) + [img_array.shape[1]]\n",
    "        cuts_done = time.time()\n",
    "        print(f'Cuts done in {cuts_done - start} seconds')\n",
    "        for i in range(len(cut_list) - 1):\n",
    "            start_cut = time.time()\n",
    "            if i > 0:\n",
    "                break\n",
    "            column_array = img_array[:, cut_list[i]:cut_list[i+1]]\n",
    "            horiz_proj = np.sum(column_array, axis=1)\n",
    "            projection_series = pd.Series(horiz_proj)\n",
    "            projection_rolling = projection_series.rolling(window=50, center=True)\n",
    "            projection_smoothed = projection_rolling.mean()\n",
    "            peaks, properties = signal.find_peaks(x=projection_smoothed, distance=50, prominence=2000)\n",
    "            slice_list = [0] + list(peaks) + [column_array.shape[0]]\n",
    "            if page % 10 == 0:\n",
    "                plt.imshow(column_array, cmap='gray')\n",
    "                for peak in peaks:\n",
    "                    plt.axhline(y=peak, color='r', linestyle='--')\n",
    "                plt.show()\n",
    "            end_cut = time.time()\n",
    "            print(f'Cut {i} done in {end_cut - start_cut} seconds')\n",
    "            for j in tqdm(range(len(slice_list) - 1)):\n",
    "                if page == 276:\n",
    "                    if i == 0 and j < 3:\n",
    "                        continue\n",
    "                    if i > 0 and j < 4:\n",
    "                        continue\n",
    "                slice_start = time.time()\n",
    "                top = slice_list[j]\n",
    "                bottom = slice_list[j+1]\n",
    "                line_height = bottom - top\n",
    "\n",
    "                if line_height < 100:\n",
    "                    gap = 100 - line_height\n",
    "                    top = max(0, top - gap//2)\n",
    "                    bottom = min(column_array.shape[0], bottom + gap//2)\n",
    "\n",
    "                # Getting image in right format for OCRs\n",
    "                line_array = column_array[top:bottom, :]\n",
    "                line_img = Image.fromarray(line_array)\n",
    "                line_img_color = line_img.convert('RGB')\n",
    "\n",
    "                # Pytesseract\n",
    "                pt_text = pt.image_to_string(line_img, config='-c tessedit_char_whitelist=0123456789QWERTYUIOPASDFGHJKLZXCVBNMqwertyuiopasdfghjklzxcvbnm,()').strip()\n",
    "                pt_text = pt_text.replace('Ww', 'W').replace('wW', 'W')\n",
    "                pt_text = re.sub(cont_pattern, '', pt_text)\n",
    "                pt_text = re.sub(symbol_pattern, '', pt_text)\n",
    "                pt_text = re.sub(see_also_pattern, replacement, pt_text)\n",
    "                pt_text = re.sub(see_pattern, replacement, pt_text)\n",
    "                pt_text = re.sub(alt_spelling_pattern, replacement_two, pt_text)\n",
    "                pt_text = pt_text.replace('(', '').replace(')', '')\n",
    "                # Getting pages\n",
    "                pt_pages = re.findall(page_pattern, pt_text)\n",
    "                pt_pages = [int(x.replace(',', '')) for x in pt_pages]\n",
    "                # Getting names\n",
    "                pt_names = re.sub(page_pattern, '', pt_text).strip().split(',')\n",
    "                pt_names = [x.replace(' ', '').capitalize() for x in pt_names]\n",
    "                while '' in pt_names:\n",
    "                    pt_names.remove('')\n",
    "                pt_done = time.time()\n",
    "                print(f'PT done in {pt_done - slice_start} seconds')\n",
    "\n",
    "                # EasyOCR\n",
    "                eo_text = eo_model.readtext(line_array, detail=0)\n",
    "                eo_text = ''.join(eo_text)\n",
    "                eo_text = eo_text.replace('Ww', 'W').replace('wW', 'W')\n",
    "                eo_text = re.sub(cont_pattern, '', eo_text)\n",
    "                eo_text = re.sub(symbol_pattern, '', eo_text)\n",
    "                eo_text = re.sub(see_also_pattern, replacement, eo_text)\n",
    "                eo_text = re.sub(see_pattern, replacement, eo_text)\n",
    "                eo_text = re.sub(alt_spelling_pattern, replacement_two, eo_text)\n",
    "                eo_text = eo_text.replace('(', '').replace(')', '')\n",
    "                # Getting pages\n",
    "                eo_pages = re.findall(page_pattern, eo_text)\n",
    "                eo_pages = [int(x.replace(',', '')) for x in eo_pages]\n",
    "                # Getting names\n",
    "                eo_names = re.sub(page_pattern, '', eo_text).strip().split(',')\n",
    "                eo_names = [x.replace(' ', '').capitalize() for x in eo_names]\n",
    "                while '' in eo_names:\n",
    "                    eo_names.remove('')\n",
    "                eo_done = time.time()\n",
    "                print(f'EO done in {eo_done - pt_done} seconds')\n",
    "\n",
    "                # TROCR\n",
    "                pixel_values = processor(line_img_color, return_tensors=\"pt\").pixel_values\n",
    "                pixel_values = pixel_values.to(device)\n",
    "                generated_ids = trocr_model.generate(pixel_values)\n",
    "                generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "                tr_text = generated_text.strip()\n",
    "                tr_text = tr_text.replace('Ww', 'W').replace('wW', 'W')\n",
    "                tr_text = re.sub(cont_pattern, '', tr_text)\n",
    "                tr_text = re.sub(symbol_pattern, '', tr_text)\n",
    "                tr_text = re.sub(see_also_pattern, replacement, tr_text)\n",
    "                tr_text = re.sub(see_pattern, replacement, tr_text)\n",
    "                tr_text = re.sub(alt_spelling_pattern, replacement_two, tr_text)\n",
    "                tr_text = tr_text.replace('(', '').replace(')', '')\n",
    "                # Getting pages\n",
    "                tr_pages = re.findall(page_pattern, tr_text)\n",
    "                tr_pages = [int(x.replace(',', '')) for x in tr_pages]\n",
    "                # Getting names\n",
    "                tr_names = re.sub(page_pattern, '', tr_text).strip().split(',')\n",
    "                tr_names = [x.replace(' ', '').capitalize() for x in tr_names]\n",
    "                while '' in tr_names:\n",
    "                    tr_names.remove('')\n",
    "                tr_done = time.time()\n",
    "                print(f'TR done in {tr_done - eo_done} seconds')\n",
    "                # Harmonizing the three predictions on page numbers\n",
    "                page_tuple_list = [tuple(pt_pages), tuple(eo_pages), tuple(tr_pages)]\n",
    "                if len(set(page_tuple_list)) == 1:\n",
    "                    pages = pt_pages\n",
    "                elif len(set(page_tuple_list)) == 2:\n",
    "                    if page_tuple_list.count(tuple(pt_pages)) > page_tuple_list.count(tuple(eo_pages)):\n",
    "                        pages = pt_pages\n",
    "                    else:\n",
    "                        pages = eo_pages\n",
    "                else:\n",
    "                    assert len(set(page_tuple_list)) == 3\n",
    "                    pages = tr_pages\n",
    "\n",
    "                # Harmonizing the three predictions on names\n",
    "                name_tuple_list = [tuple(pt_names), tuple(eo_names), tuple(tr_names)]\n",
    "                if len(set(name_tuple_list)) == 1:\n",
    "                    names = pt_names\n",
    "                elif len(set(name_tuple_list)) == 2:\n",
    "                    if name_tuple_list.count(tuple(pt_names)) > name_tuple_list.count(tuple(eo_names)):\n",
    "                        names = pt_names\n",
    "                    else:\n",
    "                        names = eo_names\n",
    "                else:\n",
    "                    assert len(set(name_tuple_list)) == 3\n",
    "                    names = tr_names\n",
    "                if len(names) == 0:\n",
    "\n",
    "                    new_pages = surname_index_df.at[surname_index_df.index[-1], 'pages'] + pages\n",
    "                    surname_index_df.at[surname_index_df.index[-1], 'pages'] = new_pages\n",
    "\n",
    "                    new_pt_pages = surname_index_df.at[surname_index_df.index[-1], 'pt_pages'] + pt_pages\n",
    "                    surname_index_df.at[surname_index_df.index[-1], 'pt_pages'] = new_pt_pages\n",
    "\n",
    "                    new_eo_pages = surname_index_df.at[surname_index_df.index[-1], 'eo_pages'] + eo_pages\n",
    "                    surname_index_df.at[surname_index_df.index[-1], 'eo_pages'] = new_eo_pages\n",
    "\n",
    "                    new_tr_pages = surname_index_df.at[surname_index_df.index[-1], 'tr_pages'] + tr_pages\n",
    "                    surname_index_df.at[surname_index_df.index[-1], 'tr_pages'] = new_tr_pages\n",
    "                    continue\n",
    "\n",
    "                new_line = pd.DataFrame({'pt_text': [pt_text], 'eo_text': [eo_text], 'tr_text': [tr_text],\n",
    "                                         'pt_pages': [pt_pages], 'eo_pages': [eo_pages], 'tr_pages': [tr_pages],\n",
    "                                         'pages': [pages], 'names': [names], 'year': [subsidy]})\n",
    "                surname_index_df = pd.concat([surname_index_df, new_line], ignore_index=True)\n",
    "                slice_done = time.time()\n",
    "                print(f'Harmonization done in {slice_done - tr_done} seconds')\n",
    "\n",
    "\n",
    "\n",
    "print(surname_index_df[['names', 'pages']])\n",
    "print(surname_index_df.iloc[0]['pages'])\n",
    "surname_index_df.to_csv(f'{PROCESSED_DATA}/surname_index.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
